# UnrealEngineGPT: Unreal Engine Conversational Large Language Model

<div align="left">

![GitHub Repo stars](https://img.shields.io/github/stars/jiayaozhang/UnrealEngineGPT?style=social)
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/Jiayao)
[![Generic badge](https://img.shields.io/badge/ğŸ”¥-Modelscope-red)](https://www.modelscope.cn/profile/Jannie)
[![Generic badge](https://img.shields.io/badge/ğŸ¼-OpenXlab-blue)](https://openxlab.org.cn/apps/detail/Janice/UnrealGPT)


</div>

<img src="img/111.png" width="450" >

Welcome to join the learning of Unreal Engine large model

* ğŸ¤—[Chinese Tutorial](README_CN.md)


## News: ğŸ˜ Open source model

**Training on 1/2*V100**ï¼ŒAfter Chinese word list expansion, incremental pre-training, and multiple rounds of instruction fine-tuning. The project also supports efficient incremental pre-training of Baichuan, Qwen, InternLM, LLaMA, Falcon and other models.
ğŸ””Using the training code of this project, and the above training data, we trained and open sourced the following model.

Modelsï¼š

| Models                                                                             | Huggingface                                | Modelscope|
|--------------------------------------------------------------------------------|-------------------------------------|------------|
| InternLM-UEGPT-7b    | [Huggingface-internLM2-7b](https://huggingface.co/Jiayao/UEGPT-internlm-7b-qlora-sft-merge/tree/main)  | [UEGPT-internLM2-7b](https://www.modelscope.cn/models/Jannie/UEGPT-internlm-7b-qlora-sft-merge/files) |  
| InternLM-UEGPT-20b   | [Huggingface-internLM2-20b](https://huggingface.co/Jiayao/UnrealGPT-internlm-20b-qlora-sft-merge) | [Modelscope-internLM2-20b](https://www.modelscope.cn/models/Jannie/UEGPT-internlm-20b-qlora-sft-merge/files)|  
| UEGPT-Datasets       | [Huggingface-UEGPT-Datasets](https://huggingface.co/datasets/Jiayao/UEGPT-Datasets)     |[Modelscope-UEGPT-Datasets](https://www.modelscope.cn/datasets/Jannie/UEGPT-Datasets/files)     |  

## Related projects

| Project                                                                                        | Description                                                                                    |
|------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| [InternLM Notes](https://github.com/jiayaozhang/InternLM_Notes)  | RAG retrieval enhanced generation supports the use of QLoRA incremental pre-training for large model inference optimization, large model parameter efficient fine-tuning (PEFT), and large model context expansion.|
| [LongQLoRA](https://github.com/yangjianxin1/LongQLoRA) | Large model length extension project can extend the length of LLaMA-13B to 8192 on a single card V100, and the performance is close to MPT-8K                                            |

## Project Description

<img src="img/buldergate.gif" width="450">

**Unreal Engine Conversational Large Language Model** is an open source Unreal Engine large language model project that supports open source models such as QLoRA and full parameter fine-tuning InternLM.

As the name of the project suggests, I hope this project can do its part for the Unreal Engine community and promote the development of games and game engines.
If your training resources are limited, we highly recommend using QLoRAâ€™s instruction fine-tuning method because we have verified the effectiveness of this method on the Open LLM Leaderboard.


ğŸ”” The main contents of this project are as followsï¼š
- ğŸ“— It supports full parameter instruction fine-tuning and QLoRA low-cost and efficient instruction fine-tuning. Among them, QLoRA is an efficient training method that we mainly recommend.
- ğŸ“— Supports most mainstream open source large models, such as Baichuan2, CodeLLaMA, LLaMA2, LLaMA, Qwen, Baichuan, ChatGLM2, InternLM, Ziya, Bloom, XVERSE, etc.
- ğŸ“— Supports weight merging of lora and base model, making reasoning more convenient.
- ğŸ“—ï¸ Model croppingï¼š[By LLMPruner: Large Language Model Clipping Tool](https://github.com/yangjianxin1/LLMPruner) ï¼Œ[Open source tailored Bloom model weights](https://huggingface.co/Jiayao) On the premise of retaining pre-trained Chinese knowledge, the amount of model parameters is effectively reduced, training costs are reduced, and training efficiency is improved.
- ğŸ“— Open source[UEGPT series instructions to fine-tune model weights](https://huggingface.co/Jiayao) ã€‚
- ğŸ“— The effectiveness of the QLoRA training process was verified on the Open LLM Leaderboard.


## Installation Environment
The versions of several major python packages are fixed under requirements.txt. Just execute the following script.

**Attentionï¼šwe are training based on torch==1.13**
```bash
pip install requirements.txt
```

<!-- ## æ¨¡å‹åˆ—è¡¨

ğŸ”” ä½¿ç”¨æœ¬é¡¹ç›®çš„è®­ç»ƒä»£ç ï¼Œä»¥åŠä¸Šè¿°è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº†ä»¥ä¸‹æ¨¡å‹ã€‚

ä¸­æ–‡æ¨¡å‹ï¼š

| æ¨¡å‹                                                                             | åŸºåº§æ¨¡å‹                                | Max Length |
|--------------------------------------------------------------------------------|-------------------------------------|------------|
| [InternLM-UEGPT-7b](https://huggingface.co/YeungNLP/firefly-baichuan2-13b) | internlm/internlm-7b     |1024     |  
| [InternLM-UEGPT-20b](https://huggingface.co/YeungNLP/firefly-baichuan-13b)   | internlm/internlm-20b      |1024     |   -->


## Training Datasets

| [moss-003-sft-data](https://huggingface.co/datasets/YeungNLP/moss-003-sft-data)                               | Chinese and English multi-round dialogue data open sourced by the MOSS team of Fudan University, containing 1 million+ data                                                                       
                                                                            
ğŸ”” Currently, this project refers to the unified data format of Chinese and English multi-round dialogue data open sourced by the MOSS team of Fudan University, and has compiled thousands of Unreal Engine Q&A multi-round data dialogues:


The training data is in jsonl format. The data format of each row is as follows. The conversation field is required and other fields can be added or deleted according to actual needs.
You can refer to the `data/dummy_data.jsonl` file in the project.


```json
  {
    "conversation_id": 390,
    "category": "Brainstorming",
    "conversation": [
      {
        "human": "å¦‚ä½•åœ¨è™šå¹»å¼•æ“ä¸­å®ç°æ°´é¢æ³¢çº¹æ•ˆæœï¼Ÿ",
        "assistant": "å®ç°æ°´é¢æ³¢çº¹æ•ˆæœåœ¨è™šå¹»å¼•æ“ä¸­é€šå¸¸æ¶‰åŠæè´¨ç¼–è¾‘å™¨çš„ä½¿ç”¨ï¼Œä»¥åŠå¯èƒ½åº”ç”¨ä¸€äº›åŠ¨æ€æ¨¡æ‹ŸæŠ€æœ¯ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºç¡€çš„æ­¥éª¤æŒ‡å—ï¼Œä»‹ç»å¦‚ä½•åœ¨è™šå¹»å¼•æ“ä¸­åˆ›å»ºæ°´é¢æ³¢çº¹æ•ˆæœï¼š\n\n### æè´¨åˆ›å»º\n\n1. **åˆ›å»ºæ°´é¢æè´¨**ï¼šé¦–å…ˆï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„æè´¨ï¼Œç”¨æ¥åº”ç”¨åˆ°ä½ çš„æ°´é¢å¯¹è±¡ä¸Šã€‚\n   \n2. **ä½¿ç”¨æ³¢çº¹çº¹ç†**ï¼šæ‰¾åˆ°é€‚åˆçš„æ³¢çº¹çº¹ç†æˆ–è€…ä½¿ç”¨ç¨‹åºç”Ÿæˆçš„æ¨¡å¼ï¼Œæ¯”å¦‚ä½¿ç”¨Perlinå™ªå£°æˆ–ç®€å•çš„æ­£å¼¦æ³¢å‡½æ•°æ¥æ¨¡æ‹Ÿæ³¢çº¹ã€‚\n\n3. **çº¹ç†åæ ‡å’Œå˜æ¢**ï¼šä½¿ç”¨æè´¨ç¼–è¾‘å™¨ä¸­çš„çº¹ç†åæ ‡ï¼ˆTexture Coordinatesï¼‰èŠ‚ç‚¹ä¸æ—¶é—´ï¼ˆTimeï¼‰èŠ‚ç‚¹ï¼Œç»“åˆä¸€äº›æ•°å­¦è¿ç®—ï¼Œæ¥å¯¹çº¹ç†è¿›è¡ŒåŠ¨æ€å˜æ¢ï¼Œäº§ç”Ÿæ³¢åŠ¨æ•ˆæœã€‚\n\n4. **åå°„ä¸æŠ˜å°„**ï¼šåˆ©ç”¨è™šå¹»çš„åå°„å’ŒæŠ˜å°„åŠŸèƒ½ä½¿æ°´é¢æ›´é€¼çœŸã€‚é€šå¸¸ï¼Œé€šè¿‡è®¾ç½®æè´¨çš„åå°„ï¼ˆSpecularï¼‰å’Œç²—ç³™åº¦ï¼ˆRoughnessï¼‰å‚æ•°ï¼Œä»¥åŠå¯èƒ½çš„æŠ˜å°„å‚æ•°æ¥æ¨¡æ‹Ÿæ°´å…‰æ•ˆæœã€‚\n\n5. **é€æ˜ä¸æ·±åº¦æ•ˆæœ**ï¼šä¸ºäº†ä½¿æ°´é¢æ›´é€¼çœŸï¼Œè¿˜éœ€è¦è°ƒæ•´é€æ˜åº¦ï¼ˆOpacityï¼‰ä»¥åŠæ·±åº¦æ•ˆæœï¼Œè¿™å¯èƒ½åŒ…æ‹¬æ°´åº•é¢œè‰²æ¸å˜ã€æ·±åº¦æ¨¡ç³Šç­‰ã€‚\n\n### åŠ¨æ€æ³¢çº¹\n\n1. **é¡¶ç‚¹ä½ç§»**ï¼šå¦‚æœéœ€è¦æ³¢çº¹å½±å“æ°´é¢çš„å½¢æ€ï¼ˆ3Dæ•ˆæœï¼‰ï¼Œå¯ä»¥ä½¿ç”¨é¡¶ç‚¹ä½ç§»ï¼ˆVertex Displacementï¼‰æŠ€æœ¯ï¼Œæ ¹æ®æ³¢çº¹çº¹ç†æ¥ä¸Šä¸‹ç§»åŠ¨æ°´é¢çš„é¡¶ç‚¹ã€‚\n\n2. **æè´¨å‚æ•°è°ƒæ•´**ï¼šåˆ›å»ºæè´¨å®ä¾‹ï¼Œå…è®¸åœ¨æ¸¸æˆè¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´æ³¢çº¹çš„å¼ºåº¦ã€é€Ÿåº¦ç­‰ã€‚\n\n3. **è§¦å‘æ³¢çº¹**ï¼šå¯¹äºäº¤äº’æ€§çš„æ³¢çº¹ï¼ˆä¾‹å¦‚ç‰©ä½“æ‰å…¥æ°´é¢ï¼‰ï¼Œéœ€è¦ç¼–å†™è“å›¾æˆ–C++ä»£ç æ¥åœ¨ç‰¹å®šä½ç½®è§¦å‘æ³¢çº¹æ•ˆæœï¼Œå¯ä»¥é€šè¿‡åœ¨æè´¨ä¸­è®¾ç½®å‚æ•°ï¼Œå¹¶ç”¨è“å›¾æˆ–ä»£ç åŠ¨æ€æ›´æ–°å®ƒä»¬æ¥å®ç°ã€‚\n\n### æ›´é«˜çº§çš„æŠ€æœ¯\n\n- **ç‰©ç†æ¨¡æ‹Ÿ**ï¼šå¯¹äºæ›´é«˜çº§çš„æ³¢çº¹æ•ˆæœï¼Œå¯ä»¥å®ç°åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿï¼Œå¦‚é‡‡ç”¨ä¸€ä¸ªåŠ¨æ€æ¨¡æ‹Ÿç³»ç»Ÿæˆ–ä½¿ç”¨é¡¶ç‚¹ç€è‰²å™¨å’Œè®¡ç®—ç€è‰²å™¨è¿›è¡Œæ¨¡æ‹Ÿã€‚\n\n- **æ¶Ÿæ¼ªæ‰©æ•£**ï¼šä¸ºäº†æ¨¡æ‹Ÿä»ä¸€ä¸ªç‚¹å‘å¤–æ‰©æ•£çš„æ¶Ÿæ¼ªæ•ˆæœï¼Œå¯ä»¥é‡‡ç”¨æŸç§æ‰©æ•£ç®—æ³•ï¼Œå¹¶å°†å…¶è¡¨ç°åœ¨æ°´é¢æè´¨ä¸Šã€‚\n\n- **å¤šå±‚æ³¢çº¹æ•ˆæœ**ï¼šé€šè¿‡å åŠ ä¸åŒçš„æ³¢çº¹å›¾å±‚æ¥åˆ›å»ºæ›´å¤æ‚çš„æ³¢çº¹æ•ˆæœã€‚\n\n- **åŠ¨æ€ç¯å¢ƒäº¤äº’**ï¼šå®ç°ç¯å¢ƒè¦ç´ ï¼ˆå¦‚é£ã€é›¨æ»´ï¼‰ä¸æ°´é¢äº¤äº’äº§ç”Ÿçš„åŠ¨æ€æ³¢çº¹æ•ˆæœã€‚\n\nåœ¨å®ç°æ°´é¢æ³¢çº¹æ•ˆæœæ—¶ï¼Œç›‘è§†æ€§èƒ½ä¹Ÿå¾ˆé‡è¦ï¼Œå› ä¸ºè¿™äº›æ•ˆæœæœ‰æ—¶ä¼šéå¸¸æ¶ˆè€—GPUèµ„æºã€‚åŠ¡å¿…ç®¡ç†å¥½æè´¨çš„å¤æ‚åº¦ï¼Œå¹¶è€ƒè™‘åªåœ¨ç©å®¶è§†é‡èŒƒå›´å†…ä½¿ç”¨æ›´é«˜çº§çš„æ•ˆæœï¼Œè€Œåœ¨è¿œå¤„é‡‡ç”¨æ›´ç®€åŒ–çš„è¡¨ç°ã€‚"
     },
     {
        "human": "è™šå¹»å¼•æ“ä¸­çš„è‡ªé€‚åº”æ›å…‰æ˜¯å¦‚ä½•è°ƒæ•´åœºæ™¯æ›å…‰çš„ï¼Ÿ",
        "assistant": "å¯¹äºå¤§é‡çš„é—®é¢˜å’Œå¹¿æ³›çš„ä¸»é¢˜ï¼Œå›ç­”æ¯ä¸ªé—®é¢˜éƒ½æ˜¯éå¸¸è€—æ—¶çš„ã€‚è¿™æ ·çš„é—®é¢˜åˆ—è¡¨çœ‹èµ·æ¥åƒæ˜¯ä¸ºäº†å¼€å‘æˆ–å­¦ä¹ è™šå¹»å¼•æ“(Unreal Engine)æ¸²æŸ“æŠ€æœ¯çš„ç›®çš„è€Œå‡†å¤‡çš„ã€‚ä¸ºäº†æä¾›ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæˆ‘å°†è¯•ç€æ€»ç»“å‡ºå…³é”®çš„å›ç­”ï¼Œè¿™æ ·å¯ä»¥æ¶µç›–å¤§éƒ¨åˆ†é—®åˆ°çš„ä¸»é¢˜ã€‚\n\n**è™šå¹»å¼•æ“æ¸²æŸ“ç®¡çº¿æ¦‚è¿°ï¼š**\nè™šå¹»å¼•æ“çš„æ¸²æŸ“ç®¡çº¿è´Ÿè´£å°†3Dåœºæ™¯è½¬æ¢ä¸ºå¹³é¢å›¾åƒã€‚ä¸»è¦æ­¥éª¤åŒ…æ‹¬åœºæ™¯å‡ ä½•ä½“çš„æå–ã€å…‰ç…§å’Œé˜´å½±çš„è®¡ç®—ã€æè´¨çš„åº”ç”¨ã€ä»¥åŠæœ€ç»ˆçš„å›¾åƒåå¤„ç†ã€‚\n\n**åˆ›å»ºå’Œç¼–è¾‘æè´¨ï¼š**\næè´¨æ˜¯å®šä¹‰å¯¹è±¡è¡¨é¢å¤–è§‚ï¼Œå¦‚é¢œè‰²ã€çº¹ç†å’Œå…‰æ³½ç­‰çš„å±æ€§ã€‚åœ¨è™šå¹»å¼•æ“ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨æè´¨ç¼–è¾‘å™¨æ¥åˆ›å»ºå’Œç¼–è¾‘æè´¨ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¯è§†åŒ–çš„èŠ‚ç‚¹ç½‘ç»œæ¥è°ƒæ•´æè´¨å±æ€§ã€‚\n\n**Post-Processingæ•ˆæœï¼š**\nåæœŸå¤„ç†æ•ˆæœæ¶‰åŠåˆ°åœ¨3Dæ¸²æŸ“åå¯¹å›¾åƒè¿›è¡Œçš„å¤„ç†ï¼Œè¿™åŒ…æ‹¬é¢œè‰²æ ¡æ­£ã€å¸ƒé²å…‰(Bloom)ã€æ™¯æ·±(Depth of Field)ã€HDRæ›å…‰ã€é•œé¢æŠ˜å°„å’Œåå°„ç­‰æ•ˆæœã€‚\n\n**è™šå¹»å¼•æ“ç²’å­ç³»ç»Ÿï¼š**\nè™šå¹»å¼•æ“æä¾›äº†å¼ºå¤§çš„Niagaraå’ŒCascadeç²’å­ç³»ç»Ÿï¼Œç”¨äºåˆ›å»ºå¦‚ç«ã€çƒŸã€é›¾ã€å°˜åŸƒç­‰æ•ˆæœã€‚è¿™å¯ä»¥é€šè¿‡è“å›¾æˆ–æè´¨ä¸ç²’å­å‚æ•°äº¤äº’è¿›è¡Œå¢å¼ºã€‚\n\n**å…‰çº¿è¿½è¸ªæŠ€æœ¯ï¼š**\nè™šå¹»å¼•æ“æ”¯æŒå®æ—¶å…‰çº¿è¿½è¸ªï¼Œå®ƒæä¾›äº†æ›´é€¼çœŸçš„å…‰å½±æ•ˆæœï¼Œåƒå‡ ä¹çœŸå®çš„åå°„ã€é˜´å½±ã€æŠ˜å°„å’Œå…¨å±€å…‰ç…§(Global Illumination, GI)ã€‚\n\n**é˜´å½±æŠ€æœ¯ï¼š**\nè™šå¹»å¼•æ“æ”¯æŒå¤šç§é˜´å½±æŠ€æœ¯ï¼ŒåŒ…æ‹¬é™æ€é˜´å½±è´´å›¾ã€åŠ¨æ€é˜´å½±å’Œè½¯é˜´å½±ã€‚è¿™äº›æŠ€æœ¯è€ƒè™‘äº†ä¸åŒå…‰æºå’Œç‰©ä½“çš„ç‰¹æ€§æ¥ç”Ÿæˆé€¼çœŸçš„é˜´å½±ã€‚\n\n**å…¨å±€å…‰ç…§è§£å†³æ–¹æ¡ˆï¼š**\nå…¨å±€å…‰ç…§æ˜¯æŒ‡å…‰åœ¨åœºæ™¯ä¸­çš„å¤šæ¬¡æ•£å°„ã€‚è™šå¹»å¼•æ“æä¾›äº†é™æ€çš„å…‰ç…§è´´å›¾ä»¥åŠåŠ¨æ€GIè§£å†³æ–¹æ¡ˆï¼Œå¦‚LPV(Light Propagation Volumes)ã€‚\n\n**å›¾åƒç©ºé—´å…‰çº¿è¿½è¸ª (ISRT) å’Œ GPUç²’å­ç³»ç»Ÿï¼š**\nISRTå…è®¸åœ¨å›¾åƒç©ºé—´æ¥æ¨¡æ‹Ÿæ›´å¤æ‚çš„å…‰å½±æ•ˆæœã€‚GPUç²’å­ç³»ç»Ÿåˆ©ç”¨äº†å›¾å½¢å¤„ç†å™¨çš„å¼ºå¤§è®¡ç®—èƒ½åŠ›ï¼Œå¯ä»¥å®ç°å¤§é‡ç²’å­å®æ—¶æ¨¡æ‹Ÿã€‚\n\n**æŠ—é”¯é½¿æŠ€æœ¯ï¼š**\nè™šå¹»å¼•æ“æ”¯æŒå¤šç§æŠ—é”¯é½¿æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„MSAAã€æ›´ç°ä»£çš„æŠ€æœ¯å¦‚FXAAã€TXAAå’ŒMLAAç­‰ã€‚\n\n**æ¸²æŸ“ä¼˜åŒ–æŠ€æœ¯ï¼š**\nä¸ºäº†æé«˜æ€§èƒ½ï¼Œè™šå¹»å¼•æ“å®ç°äº†å¤šç§ä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚LODç³»ç»Ÿï¼ˆLevel of Detailï¼Œç»†èŠ‚å±‚æ¬¡ï¼‰ã€å‰”é™¤ï¼ˆCullingï¼Œç§»é™¤è§†é‡å¤–çš„ç‰©ä½“ï¼‰ã€çº¹ç†å‹ç¼©å’Œæµå¼åŠ è½½ç­‰ã€‚\n\n**åŠ¨æ€å¤©æ°”ç³»ç»Ÿå’Œç¯å¢ƒæ•ˆæœï¼š**\nåŠ¨æ€å¤©æ°”ç³»ç»Ÿå¯ä»¥é€šè¿‡æ”¹å˜å…‰ç…§ã€ç²’å­ç³»ç»Ÿï¼ˆå¦‚ä¸‹é›¨æˆ–ä¸‹é›ªï¼‰ï¼Œä»¥åŠå¤©ç©ºç›’çš„åŠ¨æ€å˜åŒ–æ¥å®ç°ã€‚ç¯å¢ƒæ•ˆæœåˆ™å¯ä»¥é€šè¿‡ææ–™æ›´æ”¹ï¼ˆå¦‚æ¹¿åœ°æ•ˆæœï¼‰å’ŒPBRææ–™çš„é€¼çœŸæ€§æ¥æå‡ã€‚"
     },
     {
        "human": "å¦‚ä½•åœ¨è™šå¹»å¼•æ“ä¸­åˆ›å»ºåŠ¨æ€çš„ç«ç„°å’Œçˆ†ç‚¸æ•ˆæœï¼Ÿ",
        "assistant": "åˆ›å»ºå’Œç¼–è¾‘æè´¨çš„è¿‡ç¨‹ä¾èµ–äºè™šå¹»å¼•æ“çš„æè´¨ç¼–è¾‘å™¨ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç›´è§‚çš„ç•Œé¢ï¼Œå®ƒå…è®¸ç”¨æˆ·é€šè¿‡æè´¨å›¾ï¼ˆMaterial Graphï¼‰æ¥åˆ›å»ºå’Œç¼–è¾‘æè´¨ã€‚åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œæè´¨çš„å„ä¸ªéƒ¨åˆ†æ˜¯é€šè¿‡èŠ‚ç‚¹è¡¨ç¤ºçš„ã€‚æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨äº†ä¸€ç§æè´¨å±æ€§ï¼Œæ¯”å¦‚é¢œè‰²ã€çº¹ç†ã€é€æ˜åº¦ã€é«˜å…‰ç­‰ï¼Œè¿™äº›èŠ‚ç‚¹å¯ä»¥é€šè¿‡çº¿ç¼†ç›¸è¿ï¼Œä»¥å®šä¹‰æè´¨çš„æœ€ç»ˆå¤–è§‚ã€‚\n\nä»¥ä¸‹æ˜¯åˆ›å»ºå’Œç¼–è¾‘æè´¨çš„åŸºæœ¬æ­¥éª¤ï¼š\n\n1. åˆ›å»ºæ–°æè´¨ï¼š\n   - åœ¨å†…å®¹æµè§ˆå™¨ä¸­å³é”®å•å‡»ï¼Œé€‰æ‹©â€œæè´¨â€é€‰é¡¹ã€‚\n   - è¿™å°†åˆ›å»ºä¸€ä¸ªæ–°çš„æè´¨èµ„äº§ï¼Œå¹¶åœ¨æè´¨ç¼–è¾‘å™¨ä¸­æ‰“å¼€å®ƒã€‚\n\n2. ç¼–è¾‘æè´¨å±æ€§ï¼š\n   - ä½¿ç”¨æè´¨ç¼–è¾‘å™¨å·¦ä¾§çš„èŠ‚ç‚¹é¢æ¿æ¥æ·»åŠ å„ç§èŠ‚ç‚¹åˆ°æè´¨å›¾ä¸­ã€‚\n   - æ‚¨å¯ä»¥æ·»åŠ çº¹ç†é‡‡æ ·å™¨èŠ‚ç‚¹æ¥ä½¿ç”¨è´´å›¾ï¼Œå¸¸é‡èŠ‚ç‚¹æ¥å®šä¹‰é¢œè‰²æˆ–æ•°å€¼ï¼Œä»¥åŠå„ç§æè´¨å‡½æ•°èŠ‚ç‚¹æ¥å®ç°æ›´å¤æ‚çš„æ•ˆæœã€‚\n\n3. è¿æ¥èŠ‚ç‚¹ï¼š\n   - èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å®šä¹‰äº†æè´¨çš„è¿ç®—é€»è¾‘ã€‚æ‹–æ‹½èŠ‚ç‚¹ä¹‹é—´çš„é’ˆè„šæ¥è¿æ¥å®ƒä»¬ã€‚\n   - æ‚¨éœ€è¦å°†ç»“æœè¿æ¥åˆ°æè´¨çš„è¾“å‡ºèŠ‚ç‚¹ï¼Œè¿™äº›è¾“å‡ºå®šä¹‰äº†æè´¨åœ¨å…‰ç…§ã€é¢œè‰²ã€é‡‘å±åº¦ã€ç²—ç³™åº¦ç­‰æ–¹é¢çš„è¡¨ç°ã€‚\n\n4. å‚æ•°åŒ–æè´¨ï¼š\n   - é€šè¿‡æ·»åŠ æè´¨å‚æ•°ï¼Œæ¯”å¦‚â€œå‚æ•°åŒ–çº¹ç†â€ã€â€œå‚æ•°å˜é‡â€ç­‰ï¼Œæ‚¨å¯ä»¥åˆ›å»ºå¯ä»¥åœ¨è“å›¾ä¸­æˆ–åœ¨æè´¨å®ä¾‹ä¸­åŠ¨æ€è°ƒæ•´çš„æè´¨ã€‚\n   - å‚æ•°åŒ–è®©æ‚¨å¯ä»¥é‡ç”¨åŒä¸€ä¸ªæè´¨ï¼Œé€šè¿‡æ›´æ”¹å‚æ•°æ¥å®ç°ä¸åŒçš„å¤–è§‚ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚\n\n5. é¢„è§ˆå’Œè°ƒæ•´ï¼š\n   - æ‚¨å¯ä»¥å®æ—¶é¢„è§ˆæè´¨åœ¨æè´¨ç¼–è¾‘å™¨ä¸­çš„çƒä½“æˆ–ç«‹æ–¹ä½“é¢„è§ˆä¸Šçš„æ•ˆæœã€‚\n   - æ ¹æ®é¢„è§ˆè¿›è¡Œå®æ—¶è°ƒæ•´ï¼Œç›´åˆ°æ‚¨æ»¡æ„ä¸ºæ­¢ã€‚\n\n6. ä¿å­˜å¹¶åº”ç”¨ï¼š\n   - ä¸€æ—¦å®Œæˆæè´¨çš„ç¼–è¾‘ï¼Œç‚¹å‡»â€œä¿å­˜â€æŒ‰é’®æ¥ä¿å­˜æ‚¨çš„å·¥ä½œã€‚\n   - æ‚¨å¯ä»¥å°†æ­¤æè´¨æ‹–åŠ¨åˆ°åœºæ™¯ä¸­çš„å¯¹è±¡ä¸Šï¼Œæˆ–è€…åœ¨é™æ€ç½‘æ ¼çš„æè´¨æ§½ä¸­æŒ‡å®šå®ƒã€‚\n\nè¯·è®°ä½ï¼Œæè´¨åˆ¶ä½œæ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è°ƒæ•´å’Œé¢„è§ˆä»¥è¾¾åˆ°ç†æƒ³çš„æ•ˆæœã€‚è™šå¹»å¼•æ“çš„æè´¨ç¼–è¾‘å™¨æ”¯æŒPBRï¼ˆåŸºäºç‰©ç†çš„æ¸²æŸ“ï¼‰å·¥ä½œæµï¼Œä½¿å¾—åˆ›å»ºçš„æè´¨å¯ä»¥æ›´é€¼çœŸåœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„ç‰©ä½“è¡¨é¢ã€‚"
     },
     {
        "human": "è™šå¹»å¼•æ“ä¸­çš„LODç³»ç»Ÿå¦‚ä½•å½±å“æ¸²æŸ“æ€§èƒ½ï¼Ÿ",
        "assistant": "Unreal Engineï¼ˆè™šå¹»å¼•æ“ï¼‰æä¾›äº†ä¸€ç³»åˆ—åŠŸèƒ½æ¥ä¼˜åŒ–æ¸²æŸ“æ¸²æŸ“æ€§èƒ½ï¼Œå…¶ä¸­LODç³»ç»Ÿï¼ˆLevel Of Detailï¼Œè¯¦ç»†ç­‰çº§ï¼‰æ˜¯å¾ˆé‡è¦çš„ä¸€éƒ¨åˆ†ã€‚LODç³»ç»Ÿæ˜¯é€šè¿‡ä¸ºåœ¨ä¸åŒè·ç¦»ä¸Šè§‚å¯Ÿçš„å¯¹è±¡æä¾›ä¸åŒçš„ç»†èŠ‚å±‚æ¬¡ï¼Œä»è€Œä¼˜åŒ–æ¸²æŸ“æ€§èƒ½ã€‚\n\nLOD ç³»ç»Ÿå¦‚ä½•å½±å“æ¸²æŸ“æ€§èƒ½ï¼š\n\n1. **æ¨¡å‹å¤æ‚åº¦é™ä½**ï¼šå½“ç‰©ä½“è·ç¦»æ‘„åƒæœºè¾ƒè¿œæ—¶ï¼Œæ¸²æŸ“å™¨ä¼šé€‰æ‹©ä½¿ç”¨æ›´ä½ç»†èŠ‚çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ‹¥æœ‰è¾ƒå°‘çš„é¡¶ç‚¹å’Œé¢ç‰‡ï¼Œä»è€Œå‡å°‘äº†GPUçš„è´Ÿæ‹…ã€‚\n\n2. **æ¸²æŸ“è°ƒç”¨å‡å°‘**ï¼šå¦‚æœå¤šä¸ªè¿œå¤„ç‰©ä½“ä½¿ç”¨ç›¸åŒçš„LODå±‚çº§ï¼Œå¯ä»¥å°†å®ƒä»¬çš„ç»˜åˆ¶è°ƒç”¨åˆå¹¶ä¸ºå•æ¬¡è°ƒç”¨ï¼Œä»è€Œé™ä½CPUè´Ÿæ‹…ã€‚\n\n3. **çº¹ç†è´´å›¾å’Œæè´¨å¤æ‚åº¦é™ä½**ï¼šä¸æ¨¡å‹ç±»ä¼¼ï¼Œè¿œå¤„ç‰©ä½“ä¹Ÿå¯ä»¥ä½¿ç”¨æ›´ä½åˆ†è¾¨ç‡çš„çº¹ç†å’Œç®€åŒ–çš„æè´¨ï¼Œè¿™è¿›ä¸€æ­¥é™ä½äº†æ˜¾å­˜çš„ä½¿ç”¨é‡å’Œçº¹ç†é‡‡æ ·çš„æ€§èƒ½æ¶ˆè€—ã€‚\n\n4. **å½±å­åŠåå°„ä»£ç†**ï¼šé¢å‘è¿œè·ç¦»ç‰©ä½“æ—¶ï¼Œå¯ä»¥é‡‡ç”¨æ›´ä½åˆ†è¾¨ç‡çš„å½±å­è´´å›¾ï¼ˆShadow Mapsï¼‰å’Œåå°„ä»£ç†ï¼ˆReflection Probesï¼‰ï¼Œä»è€Œè¾¾åˆ°æ€§èƒ½èŠ‚çœçš„ç›®çš„ã€‚\n\n5. **å‘å¡è´Ÿè½½å‡è¡¡**ï¼šé€šè¿‡æ ¹æ®æ‘„åƒæœºçš„è§†è§’å’Œè·ç¦»è‡ªåŠ¨è°ƒæ•´æ¨¡å‹çš„ç»†èŠ‚çº§åˆ«ï¼ŒLODç³»ç»Ÿå¯ä»¥åœ¨ä¸ç‰ºç‰²å¤ªå¤šè§†è§‰æ•ˆæœçš„æƒ…å†µä¸‹å¯¹å‘å¡è´Ÿè½½è¿›è¡Œå‡è¡¡ã€‚\n\n6. **æµç•…çš„è§†è§‰ä½“éªŒ**ï¼šè®¾ç½®ä¸åŒLODé˜ˆå€¼å¯ä»¥åœ¨æ€§èƒ½å’Œè§†è§‰è´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å½“ç©å®¶åœ¨æ¸¸æˆä¸­ç§»åŠ¨æ—¶ï¼Œé€šè¿‡å¹³æ»‘çš„è½¬æ¢æ¥ç¡®ä¿ä¸ä¼šæœ‰çªå˜çš„è§†è§‰å˜åŒ–ã€‚\n\nä¸ºäº†æœ€å¤§åŒ–æ¸²æŸ“çš„æ€§èƒ½ï¼Œé€šå¸¸éœ€è¦å¯¹ç‰©ä½“è¿›è¡ŒLODçš„è®¾ç½®ï¼Œç¡®ä¿åœ¨è§†è§‰æ•ˆæœå¯æ¥å—çš„å‰æä¸‹æœ€å°åŒ–æ€§èƒ½çš„æ¶ˆè€—ã€‚Unreal Engine æä¾›äº†è‡ªåŠ¨å’Œæ‰‹åŠ¨åˆ›å»ºLODçš„å·¥å…·ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®é¡¹ç›®çš„éœ€è¦æ¥è®¾å®šä¸åŒçº§åˆ«çš„LODï¼Œä»¥è·å¾—æœ€ä½³çš„æ€§èƒ½å’Œè§†è§‰æ•ˆæœã€‚"
     }
    ]
  }
```




## Training process

<img src="img/444.png" width="450"> 


Currently, full parameter command fine-tuning and QLoRA command fine-tuning are supported. We extract various components used in training for subsequent expansion and optimization. For details, see the implementation in the component directory. Parameter configuration during training is stored in the train_args directory to facilitate unified management and changes. You can view the training configurations of different models in the train_args directory.

### Data formats
During training, we splice multiple rounds of dialogue into the following format and then tokenize. In this process, <s\>represents bos_tokenï¼Œ</s\>represents eos_tokenã€‚
```
<s>input1</s>target1</s>input2</s>target2</s>...
```

When calculating loss, we use mask. The loss in the input part does not participate in parameter update. Only the loss in the "target" part participates in parameter update. This method makes full use of the advantages of parallel computing of the model, making the training more efficient, and each target part in the multi-round dialogue participates in the training, making the training more complete. Otherwise, it is necessary to split an n-round conversation into n pieces of data, and only calculate the loss of the last target, which greatly reduces the training efficiency.


### Full parameter fine-tuning
ğŸ’» Execute the following command to fine-tune all parameters:
```bash
deepspeed --num_gpus={num_gpus} train.py --train_args_file train_args/sft.json
```

ğŸ“ train_args/sft.json. The main parameters are described as follows. The following parameters can be modified according to needs. It is recommended not to modify other parameters:

- output_dirï¼šTraining output directoryï¼Œstore checkpointã€tokenizerã€tensorboard
- model_name_or_pathï¼šThe local directory of the pretrained model, or the model name on huggingface.
- train_fileï¼šTraining data set path. You can use data/dummy_data.jsonl for debugging.
- num_train_epochsï¼šTraining rounds. If the amount of data is large enough, it is generally recommended to train for only one epoch.
- per_device_train_batch_sizeï¼šThe batch size of each graphics card.
- gradient_accumulation_stepsï¼šGradient cumulative steps. global batch=num_gpus * per_device_train_batch_size * gradient_accumulation_stepsã€‚
- gradient_checkpointingï¼šIf the memory is tight, you can turn it on. Trading time for space, the model does not cache activation status and performs two forward calculations to save memory.
- learning_rateï¼šlearning rate. When fine-tuning all parameters, it is recommended to be smaller, 1e-5 or 5e-6.
- max_seq_lengthï¼šMaximum length during training. Set it according to your own device. The longer it is, the more video memory it will take up.
- logging_stepsï¼šTrain loss is counted every few steps.
- save_stepsï¼šHow many steps to save a model.
- save_total_limitï¼šThe maximum number of checkpoints can be saved in the output_dir directory. If the checkpoint is exceeded, the oldest one will be deleted.
- lr_scheduler_typeï¼šLearning rate change strategy.
- warmup_stepsï¼šWarm up steps. How many steps does the learning rate take to increase to the specified value.
- optimï¼šOptimizer. If it is full parameter fine-tuning, it is recommended to use adamw_hf.
- seedï¼šRandom seeds are used to reproduce experimental results.
- fp16ï¼šUse fp16 mixed precision. V100 is recommended to be turned on.
- bf16ï¼šUse bf16 mixed precision. A100 is recommended to be turned on.


<img src="img/Zelda.gif" width="450"> 

### QLoRA fine-tuning

For a detailed introduction to QLoRA, please refer to the article:[ã€QLoRAå®æˆ˜ã€‘ä½¿ç”¨å•å¡é«˜æ•ˆå¾®è°ƒbloom-7b1ï¼Œæ•ˆæœæƒŠè‰³](https://mp.weixin.qq.com/s/DED7yeiE0DibsVzTmMeDOw)

QLoRA uses 4-bit nf4 quantization and adds more adapters to greatly reduce memory consumption while approaching the effect of full parameter fine-tuning as much as possible.
The QLoRA paper points out that this method can fine-tune a 33B model on a V100, and the performance is close to full parameter fine-tuning.

We use qlora on bloom-7b1. The number of parameters of the adapter is about 120 million, which exceeds the number of parameters of the bert-base model. It can be trained on V100 with a length of 1024.

ğŸ’» When using a single card, it is recommended to use this python command to start the script:
```bash
python train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json
```

ğŸ’» When using multiple cards, use the torchrun command to start the script:
```bash
torchrun --nproc_per_node={num_gpus} train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json
```

ğŸ“ The main parameters in train_args/sft-qlora.json are described as follows, which are basically the same as the parameters for full fine-tuning, with a few special ones:
- lora_rankï¼šThe rank of the qlora matrix. Generally set to 8, 16, 32, 64, etc., the author set it to 64 in the qlora paper. The larger the value, the greater the number of parameters involved in training. Generally speaking, the effect will be better, but more video memory is required.
- lora_alpha: Scaling parameters in qlora. Generally, it can be set to 16 or 32.
- lora_dropout: The dropout rate of lora weight.
- learning_rateï¼šThe learning rate setting in qlora is larger, generally 1e-4, 2e-4.

<img src="img/555.png" width="450"> 

## Model usage

### weight merging

If you use LoRA or QLoRA for training, this project only saves the weights and configuration files of the adapter, and you need to merge the adapter weights with the base model. For the script, see script/merge_lora.py

### Model reasoning
We provide scripts for single-round dialogue and multi-round dialogue. Please see the script/chat directory for details. This script is compatible with all models trained in this project for inference and is not suitable for models not trained in this project.
```bash
cd script/chat
python single_chat.py
```

The `top_p, temperature, repetition_penalty, do_sample` and other parameters in the generation script have a great impact on the generation effect of the model, and can be debugged and modified according to your own usage scenarios.

The inference script supports the use of base model and adapter for inference. The disadvantage is that each time the script is started, the weights need to be merged, which takes a long time.

Supports the use of 4bit for inference, low memory requirements, and the effect will be slightly reduced.

### Service deployment
This project supports deploying the model as an HTTP service. The script is under script/http and developed using flask. start_service.py is to start the service, and post is to send the request, which can be modified as needed.


## Generate effects

The following examples are generated by the UEGPT-InternLM2-20b model and are not modified and are for reference only.

<img src="img/fortnite.gif" width="450"> 


<details><summary><b>Unreal Engine Basis</b></summary>
<img src="img/UE.png" width="450"> 
</details>


</details>
<details><summary><b>Nanite Virtual Geometry</b></summary>
<img src="img/nanite.png" width="450"> 



</details>

<details><summary><b>Lumen illumination</b></summary>
<img src="img/lumen.png" width="450"> 


</details>

<details><summary><b>Physics Chaos</b></summary>
<img src="img/Physics.png" width="450"> 


</details>


<details><summary><b>Landscape Solutions</b></summary>
<img src="img/Landscape.png" width="450"> 

</details>


<details><summary><b>Toolchains</b></summary>
<img src="img/python.png" width="450"> 
</details>


## Large Languages Models with Unreal Engine 

ğŸ˜Stay tune!

<img src="img/chatwithtexture.gif" width="450"> 

## Installation in UE

### Setup ComfyUI

1. Install [ComfyUI](https://github.com/comfyanonymous/ComfyUI) by following the [official installation instructions](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#installing) for your OS.

2. Download **all** of the required models from the links below and place them in the corresponding ComfyUI `models` sub-directory from the list.

The models directory is relative to the ComfyUI root directory i.e. `<ComfyUI Root>/ComfyUI/models/`.

- `models/checkpoints/sd_xl_base_1.0_0.9vae.safetensors` - [Download](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors)
- `models/checkpoints/sd_xl_refiner_1.0_0.9vae.safetensors` - [Download](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/sd_xl_refiner_1.0_0.9vae.safetensors)
- `models/controlnet/diffusers_xl_canny_full.safetensors` - [Download](https://huggingface.co/lllyasviel/sd_control_collection/blob/main/diffusers_xl_canny_full.safetensors)
- `models/controlnet/diffusers_xl_depth_full.safetensors` - [Download](https://huggingface.co/lllyasviel/sd_control_collection/blob/main/diffusers_xl_depth_full.safetensors)
- `models/loras/lcm_lora_sdxl.safetensors` - [Download](https://huggingface.co/latent-consistency/lcm-lora-sdxl/blob/main/pytorch_lora_weights.safetensors) (Note: Rename the file to `lcm_lora_sdxl.safetensors`)
- `models/upscale_models/4x-UltraSharp.pth` - [Download](https://huggingface.co/lokCX/4x-Ultrasharp/blob/main/4x-UltraSharp.pth)

### Setup Unreal Engine project

1. Clone this repository.

2. Build the Unreal project by right clicking on `MyProject.uproject` and selecting `Generate Visual Studio project files`.

    This will generate a `MyProject.sln` file in the project directory. Open the file in Visual Studio and compile the project by selecting `Build -> Build Solution` in the top menu. If you are encountering errors, make sure Visual Studio is correctly configured for Unreal Engine by following [this guide](https://docs.unrealengine.com/5.3/en-US/setting-up-visual-studio-development-environment-for-cplusplus-projects-in-unreal-engine/).

3. Open the project in the Unreal Engine editor.

4. Configure the plugin by going to `Project Settings -> Plugins -> Chat With Textures`.

    If you are running ComfyUI on a remote machine, you need to set the `Comfy Url` to the correct address.

5. Open the plugin window by clicking on `Tools -> Editor Utility Widgets -> Chat With Texture`.

    If the menu item is missing you need to open the `Chat With Texture` from the Content Browser in `Plugins/Chat With Textures Content/` and click `Run Utility Widget` in the blueprint editor.

   If you don't see the Plugin folder in the Content Browser then click the `Settings` button on the top right of the Content Browser and make sure the `Show Plugin Contents` checkbox is enabled in the menu.


# Usage

Make sure ComfyUI is up and running before proceeding.

1. Select the actors you want to texture in the Outliner.

2. Set your desired settings in the Chat With Texture widget.

3. Click `Render` to start the rendering process.

Notes:
- Your meshes must have UVs (autogenerated UVs are fine).
- The plugin will automatically create a new material instance and texture for each selected actor.

## Modes

### Create

This mode uses a fast SDXL LCM model to create a low-resolution texture for each selected actor. Use this to quickly prototype and iterate on your scene.

### Refine

This mode uses a slower SDXL workflow to refine the low-resolution textures created in the `Create` mode. Use this to create the final high-resolution textures for your scene.

### Edit

This mode allows you to edit the textures created in the `Create` or `Refine` mode by using an inpainting workflow. Use this mode to fix any artifacts or errors in the textures.

You can select from two edit modes - `From Texture` and `From Object`. `From Texture` allows you to do precise edits by painting undesired areas to magenta (255, 0, 255, 255) using Mesh Paint. `From Object` will inpaint all selected actors.

## Editing the ComfyUI Workflows

You can find the ComfyUI workflows used by the plugin in the `Plugins/Chat With Textures/Content/Workflows/Original` folder. Load the JSONs into ComfyUI to see the full workflow and make changes. After making changes, save the workflow using the `Save (API Format)` button in ComfyUI and copy the JSON to the corresponding file in the `Plugins/Chat With Textures/Content/Workflows` folder.

You need to have `Enable Dev mode Options` enabled in the ComfyUI settings to see the `Save (API Format)` button.